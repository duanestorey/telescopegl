# -*- coding: utf-8 -*-

# This file is part of TelescopeGL.
# Original Telescope code by Matthew L. Bendall (https://github.com/mlbendall/telescope)
#
# New code and modifications by Duane Storey (https://github.com/duanestorey) and Claude (Anthropic).
# Licensed under MIT License.

"""TelescopeLikelihood â€” EM algorithm for resolving multi-mapped reads.

This module contains the core EM algorithm used by Telescope to assign
ambiguous RNA-seq fragments to transposable element loci.
"""
import logging as lg
from time import perf_counter

import numpy as np

from .sparse_plus import csr_matrix_plus as csr_matrix
from .backend import get_sparse_class


class TelescopeLikelihood(object):
    """EM algorithm for resolving multi-mapped fragment assignments.

    Given a sparse score matrix Q[i,j] (N fragments x K transcripts),
    iteratively estimates:
      - pi[j]: proportion of fragments from transcript j
      - theta[j]: proportion of ambiguous fragments assigned to transcript j
      - z[i,j]: posterior probability of fragment i originating from transcript j
    """

    def __init__(self, score_matrix, opts):
        # Backend-aware sparse class
        self._csr_class = get_sparse_class()

        # Raw scores
        self.raw_scores = score_matrix
        self.max_score = self.raw_scores.max()

        # N fragments x K transcripts
        self.N, self.K = self.raw_scores.shape

        # Q[i,] is the set of mapping qualities for fragment i, where Q[i,j]
        # represents the evidence for fragment i being generated by fragment j.
        # Scale the raw alignment score by the maximum alignment score
        # and multiply by a scale factor.
        self.scale_factor = 100.
        self.Q = self.raw_scores.scale().multiply(self.scale_factor).expm1()

        # z[i,] is the partial assignment weights for fragment i
        self.z = None

        self.epsilon = opts.em_epsilon
        self.max_iter = opts.max_iter

        # pi[j] is the proportion of fragments that originate from
        # transcript j. Initial value assumes equal proportions.
        self.pi = np.repeat(1./self.K, self.K)
        self.pi_init = None

        # theta[j] is the proportion of non-unique fragments reassigned
        # to transcript j. Initial value assumes equal proportions.
        self.theta = np.repeat(1./self.K, self.K)
        self.theta_init = None

        # Y[i] is the ambiguity indicator: Y[i]=1 if fragment i maps
        # to multiple transcripts, 0 otherwise. Store as N x 1 matrix.
        self.Y = (self.Q.count(1) > 1).astype(np.uint8)
        self._yslice = self.Y[:,0].nonzero()[0]

        # Log-likelihood score
        self.lnl = float('inf')

        # Prior values
        self.pi_prior = opts.pi_prior
        self.theta_prior = opts.theta_prior

        # Precalculated values
        self._weights = self.Q.max(1)
        self._total_wt = self._weights.sum()
        self._ambig_wt = self._weights.multiply(self.Y).sum()
        self._unique_wt = self._weights.multiply(1-self.Y).sum()

        # Weighted prior values
        self._pi_prior_wt = self.pi_prior * self._weights.max()
        self._theta_prior_wt = self.theta_prior * self._weights.max()
        self._pisum0 = self.Q.multiply(1-self.Y).sum(0)
        lg.debug('done initializing model')

    def estep(self, pi, theta):
        """Calculate expected values of z.

        E(z[i,j]) = ( pi[j] * theta[j]**Y[i] * Q[i,j] ) / sum_k(...)
        """
        lg.debug('started e-step')
        _amb = self._csr_class(self.Q.multiply(self.Y)).multiply(pi * theta)
        _uni = self._csr_class(self.Q.multiply(1 - self.Y)).multiply(pi)
        _n = self._csr_class(_amb + _uni)
        return _n.norm(1)

    def mstep(self, z):
        """Calculate MAP estimates for pi and theta."""
        lg.debug('started m-step')
        _weighted = z.multiply(self._weights)

        # Estimate theta_hat
        _thetasum = _weighted.multiply(self.Y).sum(0)
        _theta_denom = self._ambig_wt + self._theta_prior_wt * self.K
        _theta_hat = (_thetasum + self._theta_prior_wt) / _theta_denom

        # Estimate pi_hat
        _pisum = self._pisum0 + _thetasum
        _pi_denom = self._total_wt + self._pi_prior_wt * self.K
        _pi_hat = (_pisum + self._pi_prior_wt) / _pi_denom

        return _pi_hat.A1, _theta_hat.A1

    def calculate_lnl(self, z, pi, theta):
        """Calculate log-likelihood."""
        lg.debug('started lnl')
        _amb = self._csr_class(self.Q.multiply(self.Y)).multiply(pi * theta)
        _uni = self._csr_class(self.Q.multiply(1 - self.Y)).multiply(pi)
        _inner = self._csr_class(_amb + _uni)
        cur = z.multiply(_inner.log1p()).sum()
        lg.debug('completed lnl')
        return cur

    def em(self, use_likelihood=False, loglev=lg.WARNING, save_memory=True):
        """Run EM algorithm until convergence or max iterations."""
        inum = 0
        converged = False
        reached_max = False

        msgD = 'Iteration {:d}, diff={:.5g}'
        msgL = 'Iteration {:d}, lnl= {:.5e}, diff={:.5g}'
        while not (converged or reached_max):
            xtime = perf_counter()
            _z = self.estep(self.pi, self.theta)
            _pi, _theta = self.mstep(_z)
            inum += 1
            if inum == 1:
                self.pi_init = _pi
                self.theta_init = _theta

            diff_est = abs(_pi - self.pi).sum()

            if use_likelihood:
                _lnl = self.calculate_lnl(_z, _pi, _theta)
                diff_lnl = abs(_lnl - self.lnl)
                lg.log(loglev, msgL.format(inum, _lnl, diff_est))
                converged = diff_lnl < self.epsilon
                self.lnl = _lnl
            else:
                lg.log(loglev, msgD.format(inum, diff_est))
                converged = diff_est < self.epsilon

            reached_max = inum >= self.max_iter
            self.z = _z
            self.pi, self.theta = _pi, _theta
            lg.debug("time: {}".format(perf_counter()-xtime))

        _con = 'converged' if converged else 'terminated'
        if not use_likelihood:
            self.lnl = self.calculate_lnl(self.z, self.pi, self.theta)

        lg.log(loglev, 'EM {:s} after {:d} iterations.'.format(_con, inum))
        lg.log(loglev, 'Final log-likelihood: {:f}.'.format(self.lnl))
        return

    def reassign(self, method, thresh=0.9, initial=False):
        """Reassign fragments to expected transcripts.

        Args:
            method: One of 'exclude', 'choose', 'average', 'conf', 'unique', 'all'
            thresh: Confidence threshold for 'conf' mode
            initial: If True, use Q.norm(1) instead of z

        Returns:
            Sparse matrix where m[i,j] == 1 iff read i is reassigned to transcript j
        """
        if method not in ['exclude', 'choose', 'average', 'conf', 'unique', 'all']:
            raise ValueError('Argument "method" should be one of (exclude, choose, average, conf, unique, all)')

        _z = self.Q.norm(1) if initial else self.z

        if method == 'exclude':
            v = _z.binmax(1)
            assignments = v.multiply(v.sum(1) == 1)
        elif method == 'choose':
            v = _z.binmax(1)
            assignments = v.choose_random(1)
        elif method == 'average':
            v = _z.binmax(1)
            assignments = v.norm(1)
        elif method == 'conf':
            v = _z.threshold_filter(thresh)
            assignments = v.norm(1)
        elif method == 'unique':
            assignments = _z.multiply(1 - self.Y).ceil().astype(np.uint8)
        elif method == 'all':
            assignments = _z.indicator().astype(np.uint8)

        assignments = self._csr_class(assignments)
        return assignments

    @classmethod
    def _from_block(cls, sub_matrix, parent):
        """Create a sub-TelescopeLikelihood from a block of the parent's matrix.

        Args:
            sub_matrix: Sub-block sparse score matrix.
            parent: Parent TelescopeLikelihood instance (for opts).

        Returns:
            New TelescopeLikelihood instance for the sub-block.
        """
        class _BlockOpts:
            pass
        opts = _BlockOpts()
        opts.em_epsilon = parent.epsilon
        opts.max_iter = parent.max_iter
        opts.pi_prior = parent.pi_prior
        opts.theta_prior = parent.theta_prior
        # Ensure sub_matrix is the backend-aware sparse class
        sub_matrix = parent._csr_class(sub_matrix)
        return cls(sub_matrix, opts)

    def em_parallel(self, use_likelihood=False, loglev=lg.WARNING, n_workers=None):
        """Run EM with block decomposition for independent components.

        Decomposes the score matrix into connected components and runs EM
        on each block. Uses ThreadPoolExecutor when backend is cpu_optimized
        (Numba releases the GIL). Falls back to sequential otherwise.

        Args:
            use_likelihood: Use log-likelihood for convergence.
            loglev: Logging level.
            n_workers: Number of worker threads. Defaults to number of blocks.
        """
        from .decompose import find_blocks, split_matrix, merge_results
        from .backend import get_backend
        import scipy.sparse

        n_components, labels = find_blocks(self.raw_scores)
        lg.log(loglev, 'Block decomposition: {} components'.format(n_components))

        if n_components == 1:
            lg.log(loglev, 'Single block, falling back to standard EM')
            return self.em(use_likelihood=use_likelihood, loglev=loglev)

        blocks = split_matrix(self.raw_scores, labels, n_components)

        def _solve_block(block_info):
            sub_matrix, feat_indices, row_indices = block_info
            sub_tl = TelescopeLikelihood._from_block(sub_matrix, self)
            sub_tl.em(use_likelihood=use_likelihood, loglev=lg.DEBUG)
            return (sub_tl.pi, sub_tl.theta, sub_tl.z, sub_tl.lnl)

        backend = get_backend()
        if backend.threadsafe_parallelism:
            from concurrent.futures import ThreadPoolExecutor
            _n = n_workers or n_components
            lg.log(loglev, 'Parallel EM: {} workers for {} blocks'.format(_n, n_components))
            with ThreadPoolExecutor(max_workers=_n) as pool:
                block_results = list(pool.map(_solve_block, blocks))
        else:
            lg.log(loglev, 'Sequential EM: {} blocks'.format(n_components))
            block_results = [_solve_block(b) for b in blocks]

        # Merge results back into full-size arrays
        pi, theta, z_rows, z_cols, z_data, lnl = merge_results(
            block_results, blocks, self.K
        )

        N = self.raw_scores.shape[0]
        self.pi = pi
        self.theta = theta
        self.z = self._csr_class(
            scipy.sparse.csr_matrix(
                (z_data, (z_rows, z_cols)), shape=(N, self.K)
            )
        )
        self.lnl = lnl

        # Set pi_init from first block (approximate)
        self.pi_init = pi.copy()

        lg.log(loglev, 'EM parallel completed across {} blocks.'.format(n_components))
        lg.log(loglev, 'Final log-likelihood: {:f}.'.format(self.lnl))
        return
